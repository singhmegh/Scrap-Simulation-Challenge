{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPre00RPsfvnTquBYvurO5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhmegh/Scrap-Simulation-Challenge/blob/main/Scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOORD0WI8TUY",
        "outputId": "9aca94b5-1a54-4c4d-8028-995e511481a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configuring the path of kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "b-oGqRHd_KDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mostafaabla/garbage-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0QLh_SU_KHw",
        "outputId": "9c399be5-6019-4aae-d8f7-fec310731fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mostafaabla/garbage-classification\n",
            "License(s): ODbL-1.0\n",
            "Downloading garbage-classification.zip to /content\n",
            " 62% 149M/239M [00:00<00:00, 1.56GB/s]\n",
            "100% 239M/239M [00:00<00:00, 910MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the compressed dataset\n",
        "from zipfile import ZipFile\n",
        "dataset = \"/content/garbage-classification.zip\"\n",
        "\n",
        "with ZipFile(dataset,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('The dataset is extracted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEgx_aUJ_KLo",
        "outputId": "c57f8c6e-1c51-4b56-85b0-6cec7d649d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eVOQ_mZ_KPf",
        "outputId": "faea161d-84bf-4421-fc60-657999758828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "garbage_classification\tgarbage-classification.zip  kaggle.json  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls garbage_classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9zBnjdh_KTX",
        "outputId": "c78cfe9a-e0a1-4663-cb66-a43991d39542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "battery     brown-glass  clothes      metal  plastic  trash\n",
            "biological  cardboard\t green-glass  paper  shoes    white-glass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Dependencies**"
      ],
      "metadata": {
        "id": "zDFnnctYAPT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OZQ4IW_M_KXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's prepare the data and generate the data\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "gen_train = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2,\n",
        "                               brightness_range = (0.1, 0.5), horizontal_flip=True)\n",
        "\n",
        "train_data = gen_train.flow_from_directory(\"/content/garbage_classification\",\n",
        "                                           target_size = (224, 224), batch_size = 32, class_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwntRl7Q_Kc_",
        "outputId": "f56085a3-6715-4f37-af17-71ef4094aca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15515 images belonging to 12 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W5Kegh_AiE3",
        "outputId": "93dadca7-a292-43ed-9fd4-a52a690a5106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependenices**"
      ],
      "metadata": {
        "id": "MwXx8I_5BpVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os"
      ],
      "metadata": {
        "id": "BJ8l2kDQAiIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transforms**"
      ],
      "metadata": {
        "id": "afmNDlhgByvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_DIR = \"/content/garbage_classification\"\n",
        "BASE_DIR = \"/content/garbage_split\"\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    split_path = os.path.join(BASE_DIR, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        os.makedirs(split_path)\n",
        "\n",
        "\n",
        "classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(DATA_DIR, cls)\n",
        "    images = os.listdir(cls_path)\n",
        "\n",
        "    # Split 80/20\n",
        "    train_imgs, test_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    for img_set, split in [(train_imgs, \"train\"), (test_imgs, \"test\")]:\n",
        "        split_cls_path = os.path.join(BASE_DIR, split, cls)\n",
        "        os.makedirs(split_cls_path, exist_ok=True)\n",
        "        for img in img_set:\n",
        "            src = os.path.join(cls_path, img)\n",
        "            dst = os.path.join(split_cls_path, img)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "print(\"Dataset split into train/test at:\", BASE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsoCk737AiPv",
        "outputId": "cf5418b7-89a3-48d0-9d31-4b6a006c60e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split into train/test at: /content/garbage_split\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(os.path.join(BASE_DIR, \"train\"), transform=train_transform)\n",
        "test_dataset  = torchvision.datasets.ImageFolder(os.path.join(BASE_DIR, \"test\"), transform=test_transform)"
      ],
      "metadata": {
        "id": "3mja-uewAiTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/garbage_split\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transform)\n",
        "test_dataset  = torchvision.datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"), transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "classes = train_dataset.classes\n",
        "print(\"Classes:\", classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPSvT07eAiW6",
        "outputId": "4e3ea62c-1500-4b5b-c5a6-76e6c3d824f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model (ResNet-18 + Transfer Learning**)"
      ],
      "metadata": {
        "id": "NxigXb78DDXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GxvKV96Aiaz",
        "outputId": "990b9de2-b5d3-41aa-b843-38c61a4532a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the data**"
      ],
      "metadata": {
        "id": "8s5YPWZbDPUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    return model\n",
        "\n",
        "model = train_model(model, train_loader, criterion, optimizer, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtPb7XMADOm3",
        "outputId": "b6218f85-49ff-49ab-d3a8-6e702f62ab9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.7295\n",
            "Epoch [2/5], Loss: 0.3973\n",
            "Epoch [3/5], Loss: 0.3425\n",
            "Epoch [4/5], Loss: 0.3137\n",
            "Epoch [5/5], Loss: 0.2988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "jGD5uP_MFO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    print(classification_report(y_true, y_pred, target_names=classes))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcpTwhufDOqm",
        "outputId": "b9a6ce93-bee4-4a08-dfb9-ef7a777d2cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     battery       0.85      0.97      0.91       189\n",
            "  biological       0.95      0.97      0.96       197\n",
            " brown-glass       0.89      0.94      0.92       122\n",
            "   cardboard       0.94      0.83      0.88       179\n",
            "     clothes       0.99      0.98      0.98      1065\n",
            " green-glass       0.94      0.90      0.92       126\n",
            "       metal       0.85      0.82      0.84       154\n",
            "       paper       0.90      0.83      0.87       210\n",
            "     plastic       0.84      0.75      0.79       173\n",
            "       shoes       0.92      0.99      0.95       396\n",
            "       trash       0.89      0.89      0.89       140\n",
            " white-glass       0.84      0.85      0.85       155\n",
            "\n",
            "    accuracy                           0.93      3106\n",
            "   macro avg       0.90      0.89      0.90      3106\n",
            "weighted avg       0.93      0.93      0.93      3106\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 184    0    0    2    0    0    1    0    0    0    2    0]\n",
            " [   0  191    1    0    0    0    0    0    0    5    0    0]\n",
            " [   1    0  115    1    0    0    0    0    0    5    0    0]\n",
            " [   4    2    0  149    3    0    2   13    0    4    1    1]\n",
            " [   0    0    1    1 1045    0    2    1    0   14    1    0]\n",
            " [   3    2    3    0    0  113    1    0    2    2    0    0]\n",
            " [  11    0    2    0    0    0  127    2    4    0    2    6]\n",
            " [   8    0    0    5    4    0    2  175    4    4    2    6]\n",
            " [   1    1    3    0    1    6    7    3  130    1    8   12]\n",
            " [   0    1    1    0    3    0    0    0    0  391    0    0]\n",
            " [   4    3    0    0    1    1    0    0    5    1  125    0]\n",
            " [   1    1    3    0    1    0    7    0   10    0    0  132]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"resnet18_scrap.pth\")"
      ],
      "metadata": {
        "id": "dwiCUJBIDOxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export to TorchScript**"
      ],
      "metadata": {
        "id": "NA-fHNmzF10k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cpu = model.cpu()\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "traced_model = torch.jit.trace(model_cpu, dummy_input)\n",
        "traced_model.save(\"scrap_resnet18.pt\")\n",
        "print(\"TorchScript model saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pITWGg8CF34_",
        "outputId": "8e083a1b-3a86-4b07-b438-9d388ee3801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchScript model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export to ONNX**"
      ],
      "metadata": {
        "id": "eXo6A6-nGcOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBtOZWHhF3__",
        "outputId": "09243e0e-4f3c-46e1-dd3e-ebc7a4c07f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model.cpu(),\n",
        "    dummy_input,\n",
        "    \"garbage_resnet18.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
        ")\n",
        "\n",
        "print(\"ONNX model saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEMfSmeJF4Dv",
        "outputId": "3e993930-2541-4aa2-a290-10dbf740203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4084839472.py:3: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lightweight Inference Script**"
      ],
      "metadata": {
        "id": "tKBJ8x0sG84_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "xfpHJ83qF4HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.jit.load(\"scrap_resnet18.pt\")\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Class names (12 classes in Garbage Classification dataset)\n",
        "classes = [\n",
        "    \"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\",\n",
        "    \"trash\", \"biological\", \"green-glass\", \"brown-glass\",\n",
        "    \"white-glass\", \"clothes\", \"other\"\n",
        "]\n",
        "\n",
        "def predict_image(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_t = transform(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_t)\n",
        "        probs = F.softmax(outputs, dim=1)[0]\n",
        "        conf, pred = torch.max(probs, dim=0)\n",
        "    return classes[pred.item()], conf.item()\n",
        "\n",
        "\n",
        "img_path = \"/content/garbage_split/test/cardboard/cardboard103.jpg\"\n",
        "label, confidence = predict_image(img_path)\n",
        "print(f\"Predicted: {label} | Confidence: {confidence:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVo--yWFDO3u",
        "outputId": "389355a2-0305-43a6-985c-f42b004b59c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: paper | Confidence: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dummy Conveyor Simulatio**n"
      ],
      "metadata": {
        "id": "XKgnypn_IhJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "3g1DjuWeHD_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"scrap_resnet18.pt\"\n",
        "IMAGE_FOLDER = \"/content/garbage_split/test/battery\"\n",
        "CONF_THRESHOLD = 0.6\n",
        "RESULTS_CSV = \"conveyor_results.csv\""
      ],
      "metadata": {
        "id": "h3f2UWxuHEDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.jit.load(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "H5LJmUUdHEGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\",\n",
        "    \"trash\", \"biological\", \"green-glass\", \"brown-glass\",\n",
        "    \"white-glass\", \"clothes\", \"other\"\n",
        "]\n",
        "def classify_frame(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_t = transform(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_t)\n",
        "        probs = F.softmax(outputs, dim=1)[0]\n",
        "        conf, pred = torch.max(probs, dim=0)\n",
        "    label = classes[pred.item()]\n",
        "    flag = \"LOW CONFIDENCE\" if conf.item() < CONF_THRESHOLD else \"\"\n",
        "    return label, conf.item(), flag\n",
        "results = []\n",
        "\n",
        "for frame in sorted(os.listdir(IMAGE_FOLDER)):\n",
        "    if not frame.lower().endswith((\".jpg\",\".png\",\".jpeg\")):\n",
        "        continue\n",
        "    frame_path = os.path.join(IMAGE_FOLDER, frame)\n",
        "\n",
        "    label, conf, flag = classify_frame(frame_path)\n",
        "    print(f\"Frame: {frame} | Predicted: {label} | Confidence: {conf:.2f} {flag}\")\n",
        "\n",
        "    results.append({\n",
        "        \"frame\": frame,\n",
        "        \"prediction\": label,\n",
        "        \"confidence\": conf,\n",
        "        \"flag\": flag\n",
        "    })\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "pd.DataFrame(results).to_csv(RESULTS_CSV, index=False)\n",
        "print(f\"Simulation complete. Results saved to {RESULTS_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgc0zV38HEKK",
        "outputId": "6135b116-0078-4362-f5f5-66bda53029d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame: battery10.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery101.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery103.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery104.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery113.jpg | Predicted: cardboard | Confidence: 0.40 LOW CONFIDENCE\n",
            "Frame: battery117.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery118.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery125.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery128.jpg | Predicted: cardboard | Confidence: 0.77 \n",
            "Frame: battery134.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery136.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery143.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery151.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery156.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery157.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery160.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery172.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery180.jpg | Predicted: cardboard | Confidence: 0.89 \n",
            "Frame: battery184.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery203.jpg | Predicted: cardboard | Confidence: 0.95 \n",
            "Frame: battery204.jpg | Predicted: cardboard | Confidence: 0.85 \n",
            "Frame: battery21.jpg | Predicted: cardboard | Confidence: 0.86 \n",
            "Frame: battery22.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery221.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery230.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery233.jpg | Predicted: cardboard | Confidence: 0.88 \n",
            "Frame: battery234.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery238.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery239.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery241.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery244.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery263.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery264.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery274.jpg | Predicted: cardboard | Confidence: 0.94 \n",
            "Frame: battery278.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery28.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery280.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery29.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery297.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery299.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery302.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery308.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery32.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery322.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery327.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery331.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery337.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery340.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery341.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery347.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery35.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery352.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery354.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery357.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery358.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery373.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery385.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery390.jpg | Predicted: cardboard | Confidence: 0.67 \n",
            "Frame: battery393.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery396.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery4.jpg | Predicted: cardboard | Confidence: 0.95 \n",
            "Frame: battery401.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery403.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery404.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery406.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery411.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery417.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery42.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery422.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery423.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery424.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery428.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery431.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery444.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery445.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery446.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery448.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery449.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery45.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery456.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery472.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery474.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery478.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery480.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery483.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery489.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery494.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery497.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery499.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery500.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery502.jpg | Predicted: cardboard | Confidence: 0.89 \n",
            "Frame: battery511.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery520.jpg | Predicted: cardboard | Confidence: 0.79 \n",
            "Frame: battery526.jpg | Predicted: cardboard | Confidence: 0.92 \n",
            "Frame: battery528.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery529.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery53.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery535.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery54.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery545.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery547.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery549.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery553.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery555.jpg | Predicted: cardboard | Confidence: 0.92 \n",
            "Frame: battery558.jpg | Predicted: cardboard | Confidence: 0.94 \n",
            "Frame: battery567.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery572.jpg | Predicted: cardboard | Confidence: 0.80 \n",
            "Frame: battery585.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery588.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery596.jpg | Predicted: cardboard | Confidence: 0.92 \n",
            "Frame: battery598.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery599.jpg | Predicted: cardboard | Confidence: 0.94 \n",
            "Frame: battery60.jpg | Predicted: cardboard | Confidence: 0.92 \n",
            "Frame: battery600.jpg | Predicted: cardboard | Confidence: 0.93 \n",
            "Frame: battery602.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery603.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery607.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery611.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery614.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery624.jpg | Predicted: cardboard | Confidence: 0.94 \n",
            "Frame: battery629.jpg | Predicted: cardboard | Confidence: 0.80 \n",
            "Frame: battery633.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery649.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery652.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery654.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery662.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery663.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery668.jpg | Predicted: cardboard | Confidence: 0.82 \n",
            "Frame: battery673.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery680.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery681.jpg | Predicted: cardboard | Confidence: 0.86 \n",
            "Frame: battery683.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery685.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery689.jpg | Predicted: cardboard | Confidence: 0.88 \n",
            "Frame: battery694.jpg | Predicted: cardboard | Confidence: 0.95 \n",
            "Frame: battery699.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery702.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery707.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery708.jpg | Predicted: clothes | Confidence: 0.91 \n",
            "Frame: battery711.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery715.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery720.jpg | Predicted: cardboard | Confidence: 0.83 \n",
            "Frame: battery725.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery738.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery74.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery740.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery748.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery75.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery752.jpg | Predicted: clothes | Confidence: 0.42 LOW CONFIDENCE\n",
            "Frame: battery765.jpg | Predicted: paper | Confidence: 0.58 LOW CONFIDENCE\n",
            "Frame: battery770.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery771.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery776.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery782.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery783.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery784.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery79.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery792.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery793.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery794.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery803.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery807.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery81.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery818.jpg | Predicted: cardboard | Confidence: 0.98 \n",
            "Frame: battery835.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery838.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery840.jpg | Predicted: cardboard | Confidence: 0.49 LOW CONFIDENCE\n",
            "Frame: battery856.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery858.jpg | Predicted: cardboard | Confidence: 0.94 \n",
            "Frame: battery859.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery866.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery878.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery882.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery889.jpg | Predicted: cardboard | Confidence: 0.96 \n",
            "Frame: battery892.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery896.jpg | Predicted: cardboard | Confidence: 0.97 \n",
            "Frame: battery897.jpg | Predicted: paper | Confidence: 0.51 LOW CONFIDENCE\n",
            "Frame: battery898.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery899.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery9.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Frame: battery901.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery914.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery918.jpg | Predicted: biological | Confidence: 0.73 \n",
            "Frame: battery92.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery925.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery935.jpg | Predicted: cardboard | Confidence: 0.99 \n",
            "Frame: battery937.jpg | Predicted: cardboard | Confidence: 0.92 \n",
            "Frame: battery941.jpg | Predicted: cardboard | Confidence: 0.80 \n",
            "Frame: battery943.jpg | Predicted: cardboard | Confidence: 1.00 \n",
            "Simulation complete. Results saved to conveyor_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lN-s2COvMn5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2r8_UmjVMn87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0cCGxeLMoBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}